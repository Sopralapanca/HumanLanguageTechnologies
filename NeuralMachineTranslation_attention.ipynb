{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6jk5+1xMqxGaXKkCHu2L7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "5r9nTQOVodYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEXkqFKrpQuB",
        "outputId": "e8f4529e-a92d-4d61-ff4d-b996feeaf396"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0.7)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.47.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2YStKs6noGtt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### shape checker\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "ZAYMvbSQp6eZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET"
      ],
      "metadata": {
        "id": "Rp4TdesNqJIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the dataset, here are the steps you need to take to prepare the data:\n",
        "\n",
        "1.   Add a start and end token to each sentence.\n",
        "\n",
        "2.   Clean the sentences by removing special characters.\n",
        "3.   Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ],
      "metadata": {
        "id": "hmnSjk4Qqr6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file\n",
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "metadata": {
        "id": "6Ad99XizqOL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3f95de-a5cd-44f1-958a-1706589fd529"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp\n",
        "\n",
        "targ, inp = load_data(path_to_file)\n",
        "print(inp[-1])\n",
        "print(targ[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuKKO6mrNaK",
        "outputId": "4ad7f7a1-e655-4121-e1d1-985ac2674569"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
            "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### create tf.data DATASET\n",
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uonUBemrs8U",
        "outputId": "fc688312-570a-4634-8880-a016d66203f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xc3\\x89l record\\xc3\\xb3 a su mujer que le ten\\xc3\\xada que despertar a las siete de la ma\\xc3\\xb1ana.'\n",
            " b'No te preocupes, no estoy enfadado contigo.'\n",
            " b'Los ni\\xc3\\xb1os corrieron a la sala.' b'\\xc2\\xbfSabes usarlo?'\n",
            " b'Tom le dio apoyo moral a Mary.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'He reminded his wife to wake him up at 7:00 a.m.'\n",
            " b\"Don't worry. I'm not mad at you.\"\n",
            " b'The children ran toward the classroom.' b'Do you know how to use it?'\n",
            " b'Tom gave Mary moral support.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEXT PREPROCESSING"
      ],
      "metadata": {
        "id": "tHeztKlcsFSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text."
      ],
      "metadata": {
        "id": "InM-8QR8sPEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first step is Unicode normalization to \n",
        "# split accented characters and replace compatibility characters \n",
        "# with their ASCII equivalents.\n",
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "example_text = tf.constant('¿Todavía está en casa?')\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcghQwZ5sIDD",
        "outputId": "9ca1d441-84f8-4004-b2fc-dda63d9b7bf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Todavía está en casa?\n",
            "[START] ¿ todavia esta en casa ? [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Vectorization\n",
        "# This standardization function will be wrapped up in a tf.keras.layers.TextVectorization layer \n",
        "# which will handle the vocabulary extraction and conversion of input text to sequences of tokens.\n",
        "\n",
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "# The TextVectorization layer and many other Keras preprocessing layers have an adapt method. \n",
        "# This method reads one epoch of the training data, and works a lot like Model.fit. \n",
        "# This adapt method initializes the layer based on the data. \n",
        "# Here it determines the vocabulary:\n",
        "\n",
        "#inp is the corpus with spanish phrases\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym94_2JitzKa",
        "outputId": "d24db79d-9ee0-42ab-fb4a-af13367223e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the same for english\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GKKSencvYqE",
        "outputId": "f4c95836-5f27-454c-95a7-88be89a80eef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## these layers can convert a batch of strings into a batch of token IDs\n",
        "\n",
        "#example_input_batch is a list of strings\n",
        "\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFJ3inHcvxJ8",
        "outputId": "934a556c-5dd3-4ed2-c2f1-3b3ce8c28e6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,    7, 3535,    8,   25,  292,    5,   28,  134,    5],\n",
              "       [   2,    9,   30, 1456,   19,    9,   41, 1042,  245,    4],\n",
              "       [   2,   26,  189,    1,    8,   11,  802,    4,    3,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The get_vocabulary method can be used to convert token IDs back to text:\n",
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CcgAO40bwl9g",
        "outputId": "463448dd-43a7-4753-bd48-def8e0f5f6a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] el recordo a su mujer que le tenia que despertar a las siete de la manana . [END]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcKzy56o18ZN",
        "outputId": "8a0175b6-ec1a-4b2a-8180-925a0891b052"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   2    7 3535 ...   84    4    3]\n",
            " [   2    9   30 ...    0    0    0]\n",
            " [   2   26  189 ...    0    0    0]\n",
            " ...\n",
            " [   2   62   48 ...    0    0    0]\n",
            " [   2   13   71 ...    0    0    0]\n",
            " [   2   28 1647 ...    0    0    0]], shape=(64, 19), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENCODER/DECODER MODEL"
      ],
      "metadata": {
        "id": "SOVnmLBLyO1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "metadata": {
        "id": "jCgMM3p5z5Sz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from input_text_processor).\n",
        "2. Looks up an embedding vector for each token (Using a layers.Embedding).\n",
        "3. Processes the embeddings into a new sequence (Using a layers.GRU).\n",
        "4. Returns:\n",
        "  * The processed sequence. This will be passed to the attention head.\n",
        "  * The internal state. This will be used to initialize the decoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B3703lafz8PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The encoder returns its internal state \n",
        "# so that its state can be used to initialize the decoder.\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state\n",
        "\n",
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)"
      ],
      "metadata": {
        "id": "Pt-gilYj0Su0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention head\n",
        "\n",
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example."
      ],
      "metadata": {
        "id": "dL96NEcc7D3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "  \n",
        "attention_layer = BahdanauAttention(units)"
      ],
      "metadata": {
        "id": "_rU3bsnw7S6I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder\n",
        "\n",
        "The decoder's job is to generate predictions for the next output token.\n",
        "\n",
        "1. The decoder receives the complete encoder output.\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "4. It combines the RNN output and the context vector using Equation 3 (below) to generate the \"attention vector\".\n",
        "5. It generates logit predictions for the next token based on the \"attention vector\"."
      ],
      "metadata": {
        "id": "M40qQYIu89Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "MBKqJ5Xd_x5o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self,\n",
        "        inputs: DecoderInput,\n",
        "        state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "    shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "    if state is not None:\n",
        "      shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 1. Lookup the embeddings\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "    # Step 2. Process one step with the RNN\n",
        "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "    # Step 3. Use the RNN output as the query for the attention over the\n",
        "    # encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "    # Step 5. Generate logit predictions:\n",
        "    logits = self.fc(attention_vector)\n",
        "    shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "    return DecoderOutput(logits, attention_weights), state\n",
        "\n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "metadata": {
        "id": "AZJlX3nw9LAB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "HCOY7lDBCiLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "xzcs3NXcCmRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "NvAUX5ZuCqeU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the training step"
      ],
      "metadata": {
        "id": "DdRKWhw1Cztj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor,\n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "\n",
        "  def _preprocess(self, input_text, target_text):\n",
        "    self.shape_checker(input_text, ('batch',))\n",
        "    self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    self.shape_checker(input_tokens, ('batch', 's'))\n",
        "    self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "    self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "    target_mask = target_tokens != 0\n",
        "    self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs\n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "      self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "      self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "      # Initialize the decoder's state to the encoder's final state.\n",
        "      # This only works if the encoder and decoder have the same number of\n",
        "      # units.\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "        # Pass in two tokens from the target sequence:\n",
        "        # 1. The current input to the decoder.\n",
        "        # 2. The target for the decoder's next prediction.\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      # Average the loss over all non padding tokens.\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables\n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                  enc_output=enc_output,\n",
        "                                  mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state\n",
        "\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]), tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "jmt42tfBJxc2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test training step"
      ],
      "metadata": {
        "id": "a-3Lh40ZMH4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")\n",
        "np.log(output_text_processor.vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4mqstplMJlf",
        "outputId": "0a71d4c2-d449-40ad-8133-c240fe8f8804"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.517193191416238"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9-GAtsPMnWJ",
        "outputId": "360ce443-1f83-4557-ac39-e59832a9273d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5919304>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5599036>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.4970994>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.302761>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.5342503>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.658829>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.6373014>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1465793>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.927702>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9093168>}\n",
            "\n",
            "CPU times: user 7.93 s, sys: 356 ms, total: 8.29 s\n",
            "Wall time: 9.44 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual train"
      ],
      "metadata": {
        "id": "FocRWtDJNXxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")\n",
        "train_translator.use_tf_function = True"
      ],
      "metadata": {
        "id": "c6JnCtPLNZXF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "gWhPwH27N3nG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_translator.fit(dataset, epochs=3,\n",
        "                     callbacks=[batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGqDPAzsN6QC",
        "outputId": "1f93cb5e-08ca-4705-c3a3-d7ad37fd2ed2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1859/1859 [==============================] - 580s 308ms/step - batch_loss: 2.0408\n",
            "Epoch 2/3\n",
            "1859/1859 [==============================] - 525s 282ms/step - batch_loss: 1.0368\n",
            "Epoch 3/3\n",
            "1859/1859 [==============================] - 504s 271ms/step - batch_loss: 0.8073\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe9a0bd4c90>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "SAfdESC1xE2L",
        "outputId": "d04ab1c1-88ed-4128-f848-e46bd52f0941"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE/token')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRANSLATE"
      ],
      "metadata": {
        "id": "2SwoRSOFxJ5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the translation loop\n",
        "\n",
        "Here is a complete implementation of the text to text translation loop.\n",
        "\n",
        "This implementation collects the results into python lists, before using tf.concat to join them into tensors.\n",
        "\n",
        "This implementation statically unrolls the graph out to max_length iterations. This is okay with eager execution in python."
      ],
      "metadata": {
        "id": "Xvv9M74z80zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(result_tokens, ('batch', 't'))\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "    shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                          axis=1, separator=' ')\n",
        "    shape_checker(result_text, ('batch'))\n",
        "\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    shape_checker(result_text, ('batch',))\n",
        "    return result_text\n",
        "  \n",
        "  # This function takes the decoder's logit outputs and samples token IDs from that distribution:\n",
        "  def sample(self, logits, temperature):\n",
        "    shape_checker = ShapeChecker()\n",
        "    # 't' is usually 1 here.\n",
        "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "    shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "\n",
        "    shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "  def translate(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "\n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      # If a sequence produces an `end_token`, set it `done`\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Convert the list of generates token ids to a list of strings.\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ],
      "metadata": {
        "id": "b5Go2LkTxuze"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh5v0n2I9MHC",
        "outputId": "24fe3831-7819-4202-9f26-1001d9e036fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.', # \"This is my life.\"\"\n",
        "])\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezUYfe6w83sM",
        "outputId": "77b1430e-e548-4602-a5be-409ac594fde1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "christmas is a long time ago .\n",
            "this is my life .\n",
            "\n",
            "CPU times: user 459 ms, sys: 9.84 ms, total: 469 ms\n",
            "Wall time: 498 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZE THE PROCESS"
      ],
      "metadata": {
        "id": "3FwVfw0z95HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention weights returned by the translate method show where the model was \"looking\" when it generated each output token."
      ],
      "metadata": {
        "id": "8OH6xTbF-Aen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = result['attention'][0]\n",
        "\n",
        "print(np.sum(a, axis=-1))\n",
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "bptCqHvJ-DrK",
        "outputId": "a06d40a6-14fa-42ba-94c4-c048f0e4f15e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.        0.9999999 1.        1.        1.        1.        1.0000001\n",
            " 1.       ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.array(a), vmin=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "xLjaTFPp-Oml",
        "outputId": "ace01b38-ed59-432d-af0e-fbace705f231"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe8a4fa7d50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')\n",
        "i=0\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "Utk-leBS-UAp",
        "outputId": "5ffbc529-4ee4-42c9-f644-312850ca5199"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}